# EntraÃ®nement Rapide sur CPU (< 2 heures)

## ğŸš€ Commande OptimisÃ©e pour CPU

### Option 1 : ModÃ¨le LÃ©ger (RecommandÃ© - ~1-1.5h)

```bash
python train.py --data-dir data/images --epochs 20 --batch-size 16 --learning-rate 0.001 --device cpu --light
```

**ParamÃ¨tres :**
- `--light` : ModÃ¨le MobileNet (beaucoup plus rapide)
- `--epochs 20` : Suffisant avec learning rate plus Ã©levÃ©
- `--batch-size 16` : Optimal pour CPU (pas trop de mÃ©moire)
- `--learning-rate 0.001` : 10x plus rapide pour converger
- `--device cpu` : Forcer CPU

**Temps estimÃ© : 1-1.5 heures**

### Option 2 : ModÃ¨le Standard OptimisÃ© (~1.5-2h)

```bash
python train.py --data-dir data/images --epochs 15 --batch-size 8 --learning-rate 0.0005 --device cpu
```

**ParamÃ¨tres :**
- `--epochs 15` : Moins d'Ã©poques mais learning rate plus Ã©levÃ©
- `--batch-size 8` : Plus petit pour Ã©viter la surcharge mÃ©moire
- `--learning-rate 0.0005` : 5x plus rapide
- `--device cpu` : Forcer CPU

**Temps estimÃ© : 1.5-2 heures**

### Option 3 : Ultra Rapide (~30-45 min) - QualitÃ© rÃ©duite

```bash
python train.py --data-dir data/images --epochs 10 --batch-size 32 --learning-rate 0.002 --device cpu --light
```

**Temps estimÃ© : 30-45 minutes** (mais qualitÃ© moindre)

---

## ğŸ“Š Comparaison des Options

| Option | ModÃ¨le | Ã‰poques | Batch | LR | Temps | QualitÃ© |
|--------|--------|---------|-------|----|----|---------|
| 1 (RecommandÃ©) | LÃ©ger | 20 | 16 | 0.001 | 1-1.5h | â­â­â­â­ |
| 2 | Standard | 15 | 8 | 0.0005 | 1.5-2h | â­â­â­â­â­ |
| 3 | Ultra Rapide | 10 | 32 | 0.002 | 30-45min | â­â­â­ |

---

## âš™ï¸ Optimisations SupplÃ©mentaires

### RÃ©duire le nombre de workers (si erreurs mÃ©moire)

Modifiez temporairement `train_cv.py` ligne ~160 :
```python
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)  # Au lieu de 4
```

### Utiliser moins de donnÃ©es (pour test rapide)

CrÃ©ez un sous-dossier avec moins d'images :
```bash
# Prendre seulement 10 images par classe pour test
mkdir -p data/images_test
# Copier quelques images manuellement
python train.py --data-dir data/images_test --epochs 5 --batch-size 16 --device cpu --light
```

---

## ğŸ¯ Commande Finale RecommandÃ©e

**Pour un bon compromis vitesse/qualitÃ© :**

```bash
python train.py --data-dir data/images --epochs 20 --batch-size 16 --learning-rate 0.001 --device cpu --light
```

**Explication :**
- âœ… ModÃ¨le lÃ©ger (MobileNet) = 3-4x plus rapide
- âœ… 20 Ã©poques suffisent avec LR Ã©levÃ©
- âœ… Batch size 16 = bon pour CPU
- âœ… Learning rate 0.001 = convergence rapide
- âœ… Early stopping activÃ© (patience=10) = arrÃªt automatique si pas d'amÃ©lioration

---

## ğŸ“ Monitoring

Pendant l'entraÃ®nement, vous verrez :
- Temps par Ã©poque
- Loss et accuracy
- Early stopping si pas d'amÃ©lioration

Le modÃ¨le sera sauvegardÃ© dans `models/cv/best_model.pth` automatiquement.

---

## âš ï¸ Notes Importantes

1. **PremiÃ¨re Ã©poque plus lente** : Le chargement initial prend du temps
2. **Early Stopping** : S'arrÃªte automatiquement si pas d'amÃ©lioration pendant 10 Ã©poques
3. **MÃ©moire** : Si erreur mÃ©moire, rÃ©duisez `--batch-size` Ã  8 ou 4
4. **QualitÃ©** : Le modÃ¨le lÃ©ger est lÃ©gÃ¨rement moins performant mais beaucoup plus rapide

---

## ğŸ” VÃ©rification aprÃ¨s EntraÃ®nement

```bash
# VÃ©rifier que le modÃ¨le est crÃ©Ã©
ls -lh models/cv/best_model.pth

# Voir le rapport d'Ã©valuation
cat models/cv/evaluation_report.txt
```

